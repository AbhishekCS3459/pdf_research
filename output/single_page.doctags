<doctag><otsl><loc_107><loc_60><loc_394><loc_154><ched>Model<ecel><ched>BLEU Training Cost (FLOPs)<lcel><nl><ecel><ecel><ecel><ched>EN-DE EN-FR EN-DE EN-FR<nl><rhed>ByteNet [15] 23.75<ecel><ecel><ecel><nl><rhed>Deep-Att + PosUnk [32] 39.2<ecel><ecel><fcel>1 . 0  Â·  10 20<nl><rhed>GNMT + RL [31] 24.6 39.92<ecel><fcel>2 . 3  Â·  10 19<fcel>1 . 4  Â·  10 20<nl><rhed>ConvS2S [8] 25.16 40.46<ecel><fcel>9 . 6  Â·  10 18<fcel>1 . 5  Â·  10 20<nl><rhed>MoE [26] 26.03 40.56<ecel><fcel>2 . 0  Â·  10 19<fcel>1 . 2  Â·  10 20<nl><rhed>Deep-Att + PosUnk Ensemble [32] 40.4<ecel><ecel><fcel>8 . 0  Â·  10 20<nl><rhed>GNMT + RL Ensemble [31] 26.30 41.16<ecel><fcel>1 . 8  Â·  10 20<fcel>1 . 1  Â·  10 21<nl><rhed>ConvS2S Ensemble [8] 26.36<fcel>41.29<fcel>7 . 7  Â·  10 19<fcel>1 . 2  Â·  10 21<nl><rhed>Transformer (base model) 27.3 38.1<ecel><fcel>3 . 3 Â·  10 18<lcel><nl><rhed>Transformer (big)<fcel>28.4 41.0<fcel>2 . 3  Â·  10 19<lcel><nl><caption><loc_88><loc_45><loc_412><loc_58>Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.</caption></otsl>
<text><loc_88><loc_173><loc_411><loc_185>Label Smoothing During training, we employed label smoothing of value ls = 0 . 1 [30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</text>
<section_header_level_1><loc_89><loc_198><loc_133><loc_203>6 Results</section_header_level_1>
<section_header_level_1><loc_88><loc_214><loc_179><loc_218>6.1 Machine Translation</section_header_level_1>
<text><loc_88><loc_227><loc_412><loc_267>On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art BLEU score of 28 . 4. The configuration of this model is listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.</text>
<text><loc_88><loc_272><loc_412><loc_299>On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 , outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate PdPdrop = 0 . 1, instead of 0 . 3 .</text>
<text><loc_88><loc_303><loc_412><loc_336>For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty Î± = 0 . 6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].</text>
<text><loc_88><loc_341><loc_412><loc_367>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .</text>
<section_header_level_1><loc_88><loc_378><loc_166><loc_383>6.2 Model Variations</section_header_level_1>
<text><loc_88><loc_391><loc_412><loc_418>To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.</text>
<text><loc_88><loc_422><loc_412><loc_442>In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.</text>
<footnote><loc_99><loc_450><loc_370><loc_456>5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.</footnote>
<page_footer><loc_248><loc_469><loc_252><loc_473>8</page_footer>
</doctag>